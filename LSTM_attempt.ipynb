{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d5960d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from feature_engineering.feature_utils import create_features_from_raw_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e704f856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "csv_sequences = \"data/GDPa1_v1.2_sequences.csv\" \n",
    "csv_properties = \"data/GDPa1_v1.2_20250814.csv\"\n",
    "\n",
    "Y = \"HIC\"\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "random_seed = 42\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df = pd.read_csv(csv_sequences)\n",
    "prop_df = pd.read_csv(csv_properties)\n",
    "\n",
    "# target dataframe\n",
    "target_df = prop_df[[\"antibody_id\", Y]]\n",
    "\n",
    "# merge target and sequences\n",
    "seq_target = seq_df.merge(target_df, on=\"antibody_id\", how=\"inner\")\n",
    "sequence_features = create_features_from_raw_df(seq_target)\n",
    "# sequence_features has same index as df_raw and its own antibody_id column.\n",
    "# concatenate along columns, drop duplicate antibody_id.\n",
    "df = pd.concat(\n",
    "    [seq_target.reset_index(drop=True),\n",
    "     sequence_features.drop(columns=[\"antibody_id\"]).reset_index(drop=True)],\n",
    "    axis=1,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13e41caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### amino acid vocab, our way of scaling \n",
    "AMINO_ACIDS = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_to_idx = {aa:i for i, aa in enumerate(AMINO_ACIDS)}\n",
    "\n",
    "def one_hot_encode(seq):\n",
    "    seq = str(seq).strip().upper()\n",
    "    encoded = []\n",
    "    for aa in seq:\n",
    "        vec = [0]*20\n",
    "        if aa in aa_to_idx:\n",
    "            vec[aa_to_idx[aa]] = 1\n",
    "        encoded.append(vec)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe04cd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/94dz3nxx00n_zr937hkl8hn00000gn/T/ipykernel_72882/2590376436.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[engineered_cols] = train_df[engineered_cols].astype(float)\n",
      "/var/folders/46/94dz3nxx00n_zr937hkl8hn00000gn/T/ipykernel_72882/2590376436.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[engineered_cols] = test_df[engineered_cols].astype(float)\n"
     ]
    }
   ],
   "source": [
    "engineered_cols = [\n",
    "    \"vl_turn\",\n",
    "    \"vh_molecular_weight\",\n",
    "    \"vh_protein_sequence_length\",\n",
    "    \"vh_aromaticity\",\n",
    "    \"vh_helix\",\n",
    "    \"vh_instability\",\n",
    "    \"vh_molar_extinction_oxidized\",\n",
    "    \"vl_aromaticity\",\n",
    "    \"vh_sheet\",\n",
    "    \"G_vl_protein_sequence\",\n",
    "    \"vh_hydrophobic_count\",\n",
    "    \"vh_aromatic_count\",\n",
    "    \"vh_gravy\",\n",
    "    \"Y_vh_protein_sequence\",\n",
    "]\n",
    "\n",
    "df = df.dropna(subset=[Y]).reset_index(drop=True)\n",
    "#split train and test\n",
    "#train_df, test_df = train_test_split(df, test_size=0.2, random_state=random_seed)\n",
    "test_fold = 0\n",
    "train_df, test_df = df.loc[df['hierarchical_cluster_IgG_isotype_stratified_fold']!=test_fold], df.loc[df['hierarchical_cluster_IgG_isotype_stratified_fold']==test_fold]\n",
    "\n",
    "# set everything to the same datatype\n",
    "train_df[engineered_cols] = train_df[engineered_cols].astype(float)\n",
    "test_df[engineered_cols] = test_df[engineered_cols].astype(float)\n",
    "\n",
    "# normalize all the other features\n",
    "scaler = StandardScaler()\n",
    "train_df.loc[:, engineered_cols] = scaler.fit_transform(train_df[engineered_cols])\n",
    "test_df.loc[:, engineered_cols] = scaler.transform(test_df[engineered_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e32301ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntibodySeqDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols=None):\n",
    "        self.vh = df[\"vh_protein_sequence\"].astype(str).tolist()\n",
    "        self.vl = df[\"vl_protein_sequence\"].astype(str).tolist()\n",
    "        self.y = df[Y].astype(float).values\n",
    "        if feature_cols:\n",
    "            self.features = df[feature_cols].astype(np.float32).values\n",
    "        else:\n",
    "            self.features = None\n",
    "\n",
    "    # get length\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vh_seq = torch.tensor(one_hot_encode(self.vh[idx]), dtype=torch.float32)\n",
    "        vl_seq = torch.tensor(one_hot_encode(self.vl[idx]), dtype=torch.float32)\n",
    "        target = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "\n",
    "        sample = {\"vh_seq\": vh_seq, \"vl_seq\": vl_seq, \"target\": target}\n",
    "\n",
    "        if self.features is not None:\n",
    "            sample[\"features\"] = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "# can't use built in since sequences are not of the same length\n",
    "def collate_fn(batch):\n",
    "    vh_seqs = [b[\"vh_seq\"] for b in batch]\n",
    "    vl_seqs = [b[\"vl_seq\"] for b in batch]\n",
    "\n",
    "    batch_out = {\n",
    "        \"vh_seq\": nn.utils.rnn.pad_sequence(vh_seqs, batch_first=True, padding_value=0.0),\n",
    "        \"vl_seq\": nn.utils.rnn.pad_sequence(vl_seqs, batch_first=True, padding_value=0.0),\n",
    "        \"vh_lengths\": torch.tensor([len(s) for s in vh_seqs], dtype=torch.long),\n",
    "        \"vl_lengths\": torch.tensor([len(s) for s in vl_seqs], dtype=torch.long),\n",
    "        \"target\": torch.stack([b[\"target\"] for b in batch]),\n",
    "    }\n",
    "\n",
    "    if \"features\" in batch[0]:\n",
    "        batch_out[\"features\"] = torch.stack([b[\"features\"] for b in batch])\n",
    "\n",
    "    return batch_out\n",
    "\n",
    "\n",
    "train_dataset = AntibodySeqDataset(train_df, feature_cols=engineered_cols)\n",
    "test_dataset = AntibodySeqDataset(test_df, feature_cols=engineered_cols)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "224023c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntibodyLSTMModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size=20,\n",
    "                 hidden_size=64,\n",
    "                 num_layers=1,\n",
    "                 dropout=0.1,\n",
    "                 engineered_feat_dim=0):\n",
    "        super().__init__()\n",
    "\n",
    "        # one lstm for vh\n",
    "        self.vh_lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                               batch_first=True,\n",
    "                               dropout=dropout if num_layers > 1 else 0.0)\n",
    "\n",
    "        # two lstm for vl\n",
    "        self.vl_lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                               batch_first=True,\n",
    "                               dropout=dropout if num_layers > 1 else 0.0)\n",
    "\n",
    "        input_dim = hidden_size * 2 + engineered_feat_dim\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, vh_seq, vl_seq, vh_lengths, vl_lengths, features=None):\n",
    "\n",
    "        vh_emb = vh_seq\n",
    "        vl_emb = vl_seq\n",
    "        \n",
    "        # takes padded tensor and lengths, lets LSTM know not to consier PAD tokens\n",
    "        vh_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            vh_emb, vh_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        vl_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            vl_emb, vl_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        vh_output, (vh_hidden, vh_cell) = self.vh_lstm(vh_packed)\n",
    "        vl_output, (vl_hidden, vl_cell) = self.vl_lstm(vl_packed)\n",
    "\n",
    "        # extract final layer\n",
    "        vh_repr = vh_hidden[-1]\n",
    "        vl_repr = vl_hidden[-1]\n",
    "\n",
    "        combined = torch.cat([vh_repr, vl_repr], dim=1)  # (B, 2H)\n",
    "\n",
    "        if features is not None:\n",
    "            combined = torch.cat([combined, features], dim=1)  # (B, 2H + Feats)\n",
    "\n",
    "        # mlp scalar prediction\n",
    "        out = self.mlp(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = AntibodyLSTMModel(engineered_feat_dim=len(engineered_cols)).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e4158d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 : train MSE: 7.966 | train ρ: 0.057 || val MSE: 7.263 | val ρ: 0.131\n",
      "Epoch 02 : train MSE: 7.408 | train ρ: 0.229 || val MSE: 6.735 | val ρ: 0.162\n",
      "Epoch 03 : train MSE: 6.822 | train ρ: 0.177 || val MSE: 6.097 | val ρ: 0.152\n",
      "Epoch 04 : train MSE: 6.092 | train ρ: 0.166 || val MSE: 5.198 | val ρ: 0.122\n",
      "Epoch 05 : train MSE: 5.023 | train ρ: 0.239 || val MSE: 3.779 | val ρ: 0.102\n",
      "Epoch 06 : train MSE: 3.320 | train ρ: 0.058 || val MSE: 1.511 | val ρ: 0.116\n",
      "Epoch 07 : train MSE: 0.905 | train ρ: -0.038 || val MSE: 0.567 | val ρ: 0.139\n",
      "Epoch 08 : train MSE: 0.891 | train ρ: 0.218 || val MSE: 0.731 | val ρ: 0.139\n",
      "Epoch 09 : train MSE: 0.370 | train ρ: 0.005 || val MSE: 0.078 | val ρ: 0.149\n",
      "Epoch 10 : train MSE: 0.212 | train ρ: 0.169 || val MSE: 0.258 | val ρ: 0.188\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        vh_seq = batch[\"vh_seq\"].to(device)\n",
    "        vl_seq = batch[\"vl_seq\"].to(device)\n",
    "        vh_lengths = batch[\"vh_lengths\"].to(device)\n",
    "        vl_lengths = batch[\"vl_lengths\"].to(device)\n",
    "        targets = batch[\"target\"].to(device)\n",
    "\n",
    "        feats = batch.get(\"features\")\n",
    "        if feats is not None:\n",
    "            feats = feats.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            preds = model(vh_seq, vl_seq, vh_lengths, vl_lengths, features=feats)\n",
    "            loss = criterion(preds, targets)\n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * targets.size(0)\n",
    "        all_preds.append(preds.detach().cpu().numpy())\n",
    "        all_targets.append(targets.detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "\n",
    "    if np.std(all_preds) < 1e-8 or np.std(all_targets) < 1e-8:\n",
    "        spearman = np.nan\n",
    "    else:\n",
    "        rho, _ = spearmanr(all_targets, all_preds)\n",
    "        spearman = float(rho)\n",
    "\n",
    "    return avg_loss, spearman\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_rho = run_epoch(train_loader, train=True)\n",
    "    val_loss, val_rho = run_epoch(test_loader, train=False)\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} : \"\n",
    "        f\"train MSE: {train_loss:.3f} | train ρ: {train_rho:.3f} || \"\n",
    "        f\"val MSE: {val_loss:.3f} | val ρ: {val_rho:.3f}\"\n",
    "    )\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
